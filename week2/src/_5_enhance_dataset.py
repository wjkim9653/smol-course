import argparse
import os
import json
import random
from collections import defaultdict
from tqdm import tqdm
import numpy as np

from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import faiss

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from util import *

# âœ… ê¸°ë³¸ ì„¤ì •
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

CHOICE_LETTERS = ["A", "B", "C", "D"]

# âœ… STEP 1: ì„ ì§€ 4ê°œë§Œ ì¶”ë ¤ë‚´ê³  ì •ë‹µ ë¶„í¬ ê· í˜•í™” (A~D 4000ê°œì”© = 16,000)
def step1_filter_and_balance(input_data):
    buckets = defaultdict(list)  # ì²˜ìŒ í‚¤ ì§€ì •í•  ë•Œ ê°’ì„ ì£¼ì§€ ì•Šìœ¼ë©´ í•´ë‹¹ í‚¤ì— ëŒ€í•œ ê°’ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”

    for sample in input_data:
        if len(sample["choices"]) != 4:
            continue  # ì„ ì§€ 4ê°œì§œë¦¬ë§Œ ìœ ì§€, ê·¸ ì™¸ì˜ ê²½ìš° -> ìŠ¤í‚µ
        try:
            answer_idx = CHOICE_LETTERS.index(sample["answer"].upper())  # ìƒ˜í”Œ ì›ë³¸ì—ì„œ ì •ë‹µì˜ ì„ ì§€ ë‚´ ì¸ë±ìŠ¤(A:0, B:1, C:2, D:3...) ì²´í¬
        except ValueError:
            continue  # ì •ë‹µì´ A, B, C, D ì¤‘ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì˜ˆì™¸ ê²½ìš° -> ìŠ¤í‚µ

        # ì„ ì§€ ìˆœì„œ ëœë¤ ì„ê¸°
        paired = list(zip(CHOICE_LETTERS, sample["choices"]))  # zip() -> ("A", sample["choices"][0]), ("B", sample["choices"][1]), ("C", sample["choices"][2]), ("D", sample["choices"][3])
        random.shuffle(paired)

        new_choices = [c for _, c in paired]  # ìˆœì„œ ëœë¤ìœ¼ë¡œ ì„ì€ choiceë“¤ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë‹´ê¸°
        correct_text = sample["choices"][answer_idx]  # ì •ë‹µ ì„ ì§€ í…ìŠ¤íŠ¸
        new_answer_idx = new_choices.index(correct_text)  # ì •ë‹µ ì„ ì§€ í…ìŠ¤íŠ¸ì˜ ìƒˆë¡œìš´ (ëœë¤í™” ì´í›„) ì¸ë±ìŠ¤
        sample["choices"] = new_choices  # ëœë¤í™”í•œ ì„ ì§€ë“¤ë¡œ ë¦¬ë§¤í•‘
        sample["answer"] = CHOICE_LETTERS[new_answer_idx]  # ì‹ ê·œ ì¸ë±ìŠ¤ë¡œ ì •ë‹µ ë¦¬ë§¤í•‘
        buckets[sample["answer"]].append(sample)  # bucketì— (ì‹ ê·œ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ”) ì •ë‹µ ì„ ì§€(A,B,C,D ì¤‘) í‚¤ì˜ ë°¸ë¥˜ ë¦¬ìŠ¤íŠ¸ì— ìƒ˜í”Œ ì¶”ê°€

    # ê° ì •ë‹µ ë¬¸ìë³„ 4000ê°œê¹Œì§€ë§Œ ìœ ì§€
    balanced = []
    for letter in CHOICE_LETTERS:
        samples = buckets[letter]
        random.shuffle(samples)
        balanced.extend(samples[:4000])  # ëœë¤í™”í•œ ê° ì„ ì§€(A,B,C,D) ë³„ 4000ê°œì”© ë‹´ì•„ ì´ 16000ê°œ ì§œë¦¬ ë°¸ëŸ°ìŠ¤ë“œ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    return balanced

# âœ… STEP 2: ì§ˆë¬¸ ì¤‘ë³µ ì œê±° (ì„ë² ë”© + FAISS + ìœ ì‚¬ë„ ê¸°ì¤€)
def step2_deduplicate(samples, model):
    questions = [s["question"] for s in samples]
    embeddings = model.encode(questions, show_progress_bar=True, normalize_embeddings=True)  # normalizeí•˜ì—¬ ë‹¨ìœ„ë²¡í„°í™” -> ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ë‹¨ìˆœ ë‚´ì ìœ¼ë¡œ ê°„ì†Œí™” ê°€ëŠ¥
    # embeddingsëŠ” Nê°œ ì§ˆë¬¸ë“¤ì„ ê°ê° D ì°¨ì›ìœ¼ë¡œ ì„ë² ë”©í•œ Numpy ë°°ì—´ì´ë©°, (N, D)ì˜ shapeì„ ê°€ì§

    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)  # ë²¡í„° ë‚´ì  ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
    index.add(embeddings)

    _, neighbors = index.search(embeddings, 2)  # ìê¸° ìì‹  + ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ (top-2) ì„œì¹˜
    # neighbors -> ê° ì§ˆë¬¸ ì„ë² ë”©ë²¡í„°(Nê°œ) ë³„ë¡œ ê°€ì¥ ìœ ì‚¬í•œ 2ê°œ ë²¡í„°ë¥¼ ì°¾ìŒ -> (N, 2)ì˜ shapeì„ ê°€ì§ (neighbors[i][0] -> ìê¸°ìì‹ , neighbors[i][1] -> ìµœê·¼ì ‘ ì´ì›ƒì˜ index)
    to_remove = set()

    for i, n in enumerate(neighbors):  # neighborsëŠ” (N, 2)ì˜ shapeì„ ê°€ì§€ëŠ” Numpy ë°°ì—´ì„ì„ ê¸°ì–µ (neighbors[i][0] -> ìê¸°ìì‹ , neighbors[i][1] -> ìµœê·¼ì ‘ ì´ì›ƒì˜ index)
        if n[1] == i:  # ìµœê·¼ì ‘ ì´ì›ƒ ë˜í•œ ìê¸° ìì‹ ì´ë¼ëŠ” ë§ì€, ìœ íš¨í•œ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ì—†ëŠ” ìƒíƒœì„ì„ ì˜ë¯¸ -> ë¹„êµ ë¬´ì˜ë¯¸í•˜ë¯€ë¡œ ìŠ¤í‚µ
            continue
        sim = np.dot(embeddings[i], embeddings[n[1]])  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë‹¨ìœ„ ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì´ë¯€ë¡œ ë‹¨ìˆœ ë‚´ì )
        if sim > 0.96:  # ìœ ì‚¬ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼
            to_remove.add(i)  # ìœ ì‚¬ ì§ˆë¬¸ì€ ì‚­ì œëª©ë¡ì— ì¶”ê°€

    filtered = [s for i, s in enumerate(samples) if i not in to_remove]  # ì‚­ì œ ëª©ë¡ ì¸ë±ìŠ¤ ì œì™¸í•œ ìƒ˜í”Œë“¤ë§Œ í•„í„°ë§
    return filtered

# âœ… STEP 3: ë‚œì´ë„ ì¶”ì • (sLLMì„ ì´ìš©í•œ ì •ë‹µ ì˜ˆì¸¡ ì„±ê³µ ì—¬ë¶€)
def step3_estimate_difficulty(samples, model, tokenizer):
    annotated = []
    for sample in tqdm(samples, desc="Estimating difficulty"):
        prompt = f"Q: {sample['question']}\n"
        for i, c in enumerate(sample["choices"]):
            prompt += f"{CHOICE_LETTERS[i]}. {c}\n"
        prompt += "Answer:"

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=1)  # í—ˆìš© í† í° 1ê°œë¡œ ì œí•œí•´ ì„ ì§€ë§Œ ì¶œë ¥í•˜ë„ë¡ ìœ ë„
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        guess = generated.strip()[-1].upper()  # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹µë³€ ì„ ì§€

        # ë‚œì´ë„ ê°„ë‹¨íˆ ë§¤í•‘
        sample["difficulty"] = "hard"
        if guess == sample["answer"]:  # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹µë³€ ì„ ì§€ê°€ ì •ë‹µì¸ ê²½ìš°
            sample["difficulty"] = "easy"  # ë‚œì´ë„ ë‚®ìŒ
        elif guess in CHOICE_LETTERS:  # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹µë³€ ì„ ì§€ê°€ ì˜¤ë‹µì´ë©° ì„ ì§€ ë‚´ì— ìˆëŠ” ê²½ìš°
            sample["difficulty"] = "medium"  # ë‚œì´ë„ ì¤‘ê°„
        annotated.append(sample)
    return annotated

# âœ… STEP 4: distractor í’ˆì§ˆ í‰ê°€ (ì •ë‹µê³¼ ë„ˆë¬´ ë–¨ì–´ì§„ ì˜¤ë‹µ ì œê±°)
def step4_filter_distractors(samples, model):
    filtered = []
    for s in samples:
        correct = s["choices"][CHOICE_LETTERS.index(s["answer"])]
        good_distractors = 0
        for i, c in enumerate(s["choices"]):
            if CHOICE_LETTERS[i] == s["answer"]:
                continue
            sim = model.similarity(correct, c)  # ì‹¤ì œ ì •ë‹µì˜ ì„ë² ë”© ê°’ê³¼ ì˜¤ë‹µì— í•´ë‹¹í•˜ëŠ” distractorì˜ ì„ë² ë”© ê°’ ê°„ì˜ ìœ ì‚¬ë„
            if sim > 0.3:  # ì •ë‹µê³¼ì˜ ìœ ì‚¬ë„ê°€ ë†’ê²Œ ë‚˜ì˜¤ëŠ” í›Œë¥­í•œ distractorì¸ ê²½ìš°
                good_distractors += 1
        if good_distractors >= 2:  # ìµœì†Œ 2ê°œ distractorëŠ” ê´œì°®ì•„ì•¼ í†µê³¼
            filtered.append(s)  # ì„ ì§€ì— ìˆëŠ” ì˜¤ë‹µ ì¤‘ good distractorì— í•´ë‹¹í•˜ëŠ” ê²ƒì´ 2ê°œ ë¯¸ë§Œì¸ ê²½ìš°ëŠ” í•„í„°ë§í•´ ìƒ˜í”Œì—ì„œ ì œì™¸
    return filtered

# âœ… STEP 5: ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§ (MMR ë°©ì‹ìœ¼ë¡œ 10,000ê°œ ì„ íƒ)
def step5_select_final(samples, model, k=10000):
    embeddings = model.encode([s["question"] for s in samples], normalize_embeddings=True)

    selected = []
    selected_set = set()
    sims = np.inner(embeddings, embeddings)  # cosine similarity matrix, (N, N) shape
    scores = sims.mean(axis=1)  # ê° ì§ˆë¬¸ì´ ì „ì²´ ì§ˆë¬¸ì— ëŒ€í•´ ê°–ëŠ” ìœ ì‚¬ë„ í‰ê·  í–‰ë ¬, (N, 1) shape => í’ˆì§ˆ ì ìˆ˜ (Relevance Score in MMR)

    ranked = list(np.argsort(-scores))  # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    selected.append(ranked[0])  # ì²« ì§ˆë¬¸ì€ ê°€ì¥ ë†’ì€ relevance score ê°€ì§„ ìƒ˜í”Œ
    selected_set.add(ranked[0])

    while len(selected) < k:  # kê°œ ìƒ˜í”Œê¹Œì§€ ë°˜ë³µ ìƒ˜í”Œë§
        mmr_scores = []
        for i in range(len(samples)):
            if i in selected_set:
                continue
            relevance = scores[i]
            diversity = max([sims[i][j] for j in selected])  # ì´ë¯¸ ì„ íƒë˜ì–´ ìˆëŠ” ìƒ˜í”Œë“¤ê³¼ì˜ ìœ ì‚¬ë„ ì¤‘ ê°€ì¥ í° ê°’ì´ diversityê°’ì´ ë¨
            mmr = 0.7 * relevance - 0.3 * diversity  # MMR ê°€ì¤‘ì¹˜ëŠ” 0.7ë¡œ, MMRì€ ì§ˆë¬¸ì˜ ì¢‹ì€ ì •ë„(relevance) - ì§ˆë¬¸ì´ ê¸°ì¡´ ì§ˆë¬¸ê³¼ ë¹„ìŠ·í•œ ì •ë„(diversity)ë¡œ ì‚°ì¶œ (ê°€ì¤‘ì¹˜ëŠ” 0.7, (1-0.7)ë¡œ ì¡ìŒ)
            mmr_scores.append((mmr, i))
        mmr_scores.sort(reverse=True)  # ë†’ì€ MMR ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        _, best = mmr_scores[0]  # ê°€ì¥ ë†’ì€ MMR ì ìˆ˜ë¥¼ ë°›ì€ ìƒ˜í”Œ 1ê°œë¥¼ ì„ ì •í•´ selectedì— ì¶”ê°€
        selected.append(best)
        selected_set.add(best)

    final = [samples[i] for i in selected]
    return final

# âœ… ì„ë² ë”© ìœ í‹¸ í´ë˜ìŠ¤
class EmbeddingModel:
    def __init__(self, model_name="BAAI/bge-small-en-v1.5"):
        self.model = SentenceTransformer(model_name)

    def encode(self, texts, **kwargs):
        return self.model.encode(texts, **kwargs)

    def similarity(self, a, b):
        vecs = self.encode([a, b], normalize_embeddings=True)
        return np.dot(vecs[0], vecs[1])

# âœ… sLLM ë¡œë”© í•¨ìˆ˜
def load_sllm(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name, device_map="auto", torch_dtype=torch.float16
    )
    return tokenizer, model

# âœ… MAIN
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True)
    parser.add_argument("--intermediate_dir", required=True)
    parser.add_argument("--output", required=True)
    args = parser.parse_args()

    os.makedirs(args.intermediate_dir, exist_ok=True)

    print("ğŸ”¹ Loading input data...")
    data = load_jsonl(args.input)

    print("ğŸ”¹ Step 1: Filtering 4-choice & balancing answers...")
    data1 = step1_filter_and_balance(data)
    save_jsonl(data1, os.path.join(args.intermediate_dir, "step1_balanced.jsonl"))

    print("ğŸ”¹ Step 2: Removing duplicates...")
    embed_model = EmbeddingModel()
    data2 = step2_deduplicate(data1, embed_model)
    save_jsonl(data2, os.path.join(args.intermediate_dir, "step2_dedup.jsonl"))

    print("ğŸ”¹ Step 3: Estimating difficulty using sLLM...")
    # tokenizer, sllm = load_sllm("meta-llama/Meta-Llama-3-8B-Instruct")
    tokenizer, sllm = load_sllm("Qwen/Qwen3-0.6B")
    data3 = step3_estimate_difficulty(data2, sllm, tokenizer)
    save_jsonl(data3, os.path.join(args.intermediate_dir, "step3_difficulty.jsonl"))

    print("ğŸ”¹ Step 4: Filtering low-quality distractors...")
    data4 = step4_filter_distractors(data3, embed_model)
    save_jsonl(data4, os.path.join(args.intermediate_dir, "step4_distractor_filtered.jsonl"))

    print("ğŸ”¹ Step 5: Selecting final 10k with diversity...")
    data5 = step5_select_final(data4, embed_model, k=10000)
    save_jsonl(data5, args.output)

    print(f"âœ… Done! Final dataset saved to: {args.output}")

if __name__ == "__main__":
    main()